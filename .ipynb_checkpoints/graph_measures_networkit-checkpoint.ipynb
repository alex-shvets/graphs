{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkit\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import itertools\n",
    "\n",
    "def change_pickle_protocol(filepath,protocol=2):\n",
    "    with open(filepath,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    with open(filepath,'wb') as f:\n",
    "        pickle.dump(obj,f,protocol=protocol)\n",
    "\n",
    "def get_graph_measures(max_cc_gcommon):\n",
    "    summary = {}\n",
    "    print(str(datetime.datetime.now()))\n",
    "    diam = networkit.distance.Diameter(max_cc_gcommon, networkit.distance.DiameterAlgo.Exact).run().getDiameter()\n",
    "    print(\"diam\", diam[0])\n",
    "    summary[\"diameter\"] = diam[0]\n",
    "\n",
    "    print(str(datetime.datetime.now()))\n",
    "    eff_diam_max_cc = networkit.distance.EffectiveDiameter(max_cc_gcommon).run().getEffectiveDiameter()\n",
    "    print(\"eff_diam\", eff_diam_max_cc)\n",
    "    summary[\"eff_diameter\"] = eff_diam_max_cc\n",
    "\n",
    "    print(str(datetime.datetime.now()))\n",
    "    communities = networkit.community.detectCommunities(max_cc_gcommon)\n",
    "    modularity = networkit.community.Modularity(max_cc_gcommon).getQuality(communities, max_cc_gcommon)\n",
    "    print(\"communities (num, max, avg)\", communities.numberOfSubsets(), max(communities.subsetSizes()), 1.0*sum(communities.subsetSizes())/len(communities.subsetSizes()))\n",
    "    print(\"modularity\", modularity)\n",
    "    summary[\"communities_num\"] = communities.numberOfSubsets()\n",
    "    summary[\"communities_max_size\"] = max(communities.subsetSizes())\n",
    "    summary[\"communities_avg_size\"] = 1.0*sum(communities.subsetSizes())/len(communities.subsetSizes())\n",
    "    summary[\"communities_rel_max_size\"] = 1.0*max(communities.subsetSizes())/communities.numberOfElements()\n",
    "    summary[\"communities_rel_avg_size\"] = (1.0*sum(communities.subsetSizes())/len(communities.subsetSizes()))/communities.numberOfElements()\n",
    "    summary[\"communities_modularity\"] = modularity\n",
    "\n",
    "    print(str(datetime.datetime.now()))\n",
    "    dc = sorted(networkit.centrality.DegreeCentrality(max_cc_gcommon, normalized=True).run().scores(), reverse=True)\n",
    "    num_nodes_high_dc = len(dc)\n",
    "    for i in range(len(dc)):\n",
    "        if dc[i]<(dc[0]*0.1):\n",
    "            num_nodes_high_dc = i+1\n",
    "            break\n",
    "    print(\"degree_centrality (max, avg, num_high, rel_num_high)\", dc[0], 1.0*sum(dc)/len(dc), num_nodes_high_dc, 1.0*num_nodes_high_dc/len(dc))\n",
    "    summary[\"max_degc\"] = dc[0]\n",
    "    summary[\"avg_degc\"] = 1.0*sum(dc)/len(dc)\n",
    "    summary[\"num_nodes_high_degc\"] = num_nodes_high_dc\n",
    "    summary[\"rel_num_nodes_high_degc\"] = 1.0*num_nodes_high_dc/len(dc)\n",
    "\n",
    "    print(str(datetime.datetime.now()))\n",
    "    btw = networkit.centrality.ApproxBetweenness(max_cc_gcommon).run()\n",
    "    btwc = sorted(btw.scores(), reverse = True)\n",
    "    summary[\"max_btwc\"] = btwc[0]\n",
    "    summary[\"avg_btwc\"] = 1.0*sum(btwc)/len(btwc)\n",
    "    high_btwc = len(list(x for x in btwc if x>0))\n",
    "    summary[\"num_nodes_high_btwc\"] = high_btwc\n",
    "    summary[\"rel_num_nodes_high_btwc\"] = 1.0*high_btwc/len(btwc)\n",
    "    print(\"betwenness centrality (max, avg, high, rel_high)\", summary[\"max_btwc\"], summary[\"avg_btwc\"], summary[\"num_nodes_high_btwc\"], summary[\"rel_num_nodes_high_btwc\"])\n",
    "\n",
    "    print(str(datetime.datetime.now()))\n",
    "    eig = networkit.centrality.EigenvectorCentrality(max_cc_gcommon).run()\n",
    "    eigc = sorted(eig.scores(), reverse = True)\n",
    "    summary[\"max_eigc\"] = eigc[0]\n",
    "    summary[\"avg_eigc\"] = 1.0*sum(eigc)/len(eigc)\n",
    "    high_eigc = len(list(x for x in eigc if x>0.01))\n",
    "    summary[\"num_nodes_high_eigc\"] = high_eigc\n",
    "    summary[\"rel_num_nodes_high_eigc\"] = 1.0*high_eigc/len(eigc)\n",
    "    print(\"eigenvector centrality (max, avg, high, rel_high)\", summary[\"max_eigc\"], summary[\"avg_eigc\"], summary[\"num_nodes_high_eigc\"], summary[\"rel_num_nodes_high_eigc\"])\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load graphlab.SGraph from csv\n",
    "graphid = 0\n",
    "path_to_largest_connected_component = \"max_cc_graph_\"+str(graphid)\n",
    "df_v = pd.read_csv(path_to_largest_connected_component+\"/vertices.csv\")\n",
    "df_e = pd.read_csv(path_to_largest_connected_component+\"/edges.csv\")\n",
    "transf_id = dict(list((df_v[\"__id\"][i], i) for i in range(len(df_v[\"__id\"]))))\n",
    "max_cc = networkit.graph.Graph(len(df_v[\"__id\"]), directed=False)\n",
    "for i in range(len(df_e[\"__src_id\"])):\n",
    "    max_cc.addEdge(transf_id[df_e[\"__src_id\"][i]], transf_id[df_e[\"__dst_id\"][i]])\n",
    "summary = get_graph_measures(max_cc)\n",
    "summary[\"graph_id\"] = int(graphid)\n",
    "\n",
    "df_main = pd.DataFrame()\n",
    "ss = pd.Series(summary, name=str(graphid))\n",
    "df_main = df_main.append(ss)\n",
    "summ_ntwk = \"summary_networkit.pckl\"\n",
    "df_main.to_pickle(summ_ntwk)\n",
    "change_pickle_protocol(summ_ntwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(summ_ntwk, graph_ids):\n",
    "\n",
    "    upfolder_ccs = \"max_ccs\"\n",
    "    df_main = pd.DataFrame()\n",
    "    if os.path.exists(summ_ntwk):\n",
    "        df_main = pd.read_pickle(summ_ntwk)\n",
    "\n",
    "    for graphid in graph_ids:\n",
    "        try:\n",
    "            if(\"graph_id\" in df_main)and(int(graphid) in list(map(int,df_main[\"graph_id\"].tolist()))):\n",
    "                print(graphid, \"graph has already been analyzed\")\n",
    "                continue\n",
    "            gid_str = str(graphid)\n",
    "            print(\"analysis of the graph %s\" % gid_str)\n",
    "            df_v = pd.read_csv(upfolder_ccs+\"/max_cc_\"+(\"graph_%s/vertices.csv\" % gid_str))\n",
    "            df_e = pd.read_csv(upfolder_ccs+\"/max_cc_\"+(\"graph_%s/edges.csv\" % gid_str))\n",
    "            transf_id = dict(list((df_v[\"__id\"][i], i) for i in range(len(df_v[\"__id\"]))))\n",
    "            max_cc_gcommon = networkit.graph.Graph(len(df_v[\"__id\"]), directed=False)\n",
    "            for i in range(len(df_e[\"__src_id\"])):\n",
    "                max_cc_gcommon.addEdge(transf_id[df_e[\"__src_id\"][i]], transf_id[df_e[\"__dst_id\"][i]])\n",
    "            print(\"graph was loaded, graph %s\" % gid_str)\n",
    "            summary = get_graph_measures(max_cc_gcommon)\n",
    "            summary[\"graph_id\"] = int(graphid)\n",
    "\n",
    "            ss = pd.Series(summary, name=str(graphid))\n",
    "            df_main = df_main.append(ss)\n",
    "            df_main.to_pickle(summ_ntwk)\n",
    "            change_pickle_protocol(summ_ntwk)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main(summ_ntwk = \"summary_networkit.pckl\", graph_ids = range(10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
